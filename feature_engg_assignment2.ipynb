{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c16eb88-cbb7-4fd3-ac42-29372cd37c6f",
   "metadata": {},
   "source": [
    "# ANSWER 1\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale numerical features to a specific range, typically between 0 and 1. It transforms the original data so that the minimum value of the feature becomes 0, and the maximum value becomes 1, while preserving the relative relationships between the data points. Min-Max scaling is useful when features have different scales and you want to bring them all to a common scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b2bd24d-c62d-4699-93b3-8586268ad496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.23529412]\n",
      " [0.47058824]\n",
      " [0.70588235]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = {\n",
    "    'Area': [800, 1200, 1600, 2000, 2500]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1e009f-43a5-437c-b190-14c951b79dd5",
   "metadata": {},
   "source": [
    "# ANSWER 2\n",
    "The Unit Vector technique, also known as normalization, is a feature scaling method that scales the data to have a magnitude of 1 (unit length). It involves dividing each data point by the magnitude of the feature vector. This method is commonly used in machine learning algorithms that rely on distances between data points, such as clustering or nearest neighbors.\n",
    "## Unit Vector technique Different from Min-Max Scaling:\n",
    "The primary difference is that Min-Max scaling scales the data to a specific range (e.g., 0 to 1), while the Unit Vector technique scales the data so that all feature vectors have a magnitude of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92558446-dc55-49d3-89d9-d686217983e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.72819999 0.6853647 ]\n",
      " [0.71500667 0.69911763]\n",
      " [0.6804511  0.73279349]\n",
      " [0.71483403 0.69929416]\n",
      " [0.7151872  0.69893295]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer\n",
    "data = {'Math Score': [85, 90, 78, 92, 88],'English Score': [80, 88, 84, 90, 86]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Unit Vector technique\n",
    "normalizer = Normalizer()\n",
    "normalized_data = normalizer.fit_transform(df)\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a692e330-07a4-4198-8fda-cdefb58e283f",
   "metadata": {},
   "source": [
    "# ANSWER 3\n",
    "PCA is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while retaining as much variance as possible. It achieves this by identifying the principal components, which are orthogonal directions in the original feature space, along which the data varies the most. These principal components are used to represent the data in a lower-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e65e227-24e2-430c-bc67-ffb63b4d53bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.89897949e+00  3.84592537e-16]\n",
      " [ 2.44948974e+00 -1.28197512e-16]\n",
      " [-0.00000000e+00 -0.00000000e+00]\n",
      " [-2.44948974e+00  1.28197512e-16]\n",
      " [-4.89897949e+00  2.56395025e-16]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "data = {\n",
    "    'Feature1': [1, 2, 3, 4, 5],\n",
    "    'Feature2': [5, 4, 3, 2, 1],\n",
    "    'Feature3': [10, 8, 6, 4, 2]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(df)\n",
    "print(reduced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05dbf78-0830-4f63-991b-d9898aa8670f",
   "metadata": {},
   "source": [
    "# ANSWER 4\n",
    "PCA can be used for feature extraction by identifying the most important patterns (principal components) in the data. Instead of using the original features, we can represent the data using these principal components, which are a linear combination of the original features. This reduces the dimensionality of the data while preserving most of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7de1614a-5b81-482a-8524-13c6699a88c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.47213595]\n",
      " [ 2.23606798]\n",
      " [-0.        ]\n",
      " [-2.23606798]\n",
      " [-4.47213595]]\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'X': [1, 2, 3, 4, 5],\n",
    "    'Y': [2, 4, 6, 8, 10]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply PCA for feature extraction\n",
    "pca = PCA(n_components=1)\n",
    "extracted_feature = pca.fit_transform(df)\n",
    "\n",
    "print(extracted_feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cdfa34-cadd-46f6-ac97-2cf663132e5f",
   "metadata": {},
   "source": [
    "# ANSWER 5\n",
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess features like \"price,\" \"rating,\" and \"delivery time\" to ensure that they are on a common scale for more effective recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db33ca9e-10fd-473c-b4fa-adb7e81a8744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.625 0.   ]\n",
      " [0.5   0.125 0.5  ]\n",
      " [0.25  1.    0.25 ]\n",
      " [0.75  0.    1.   ]\n",
      " [1.    0.375 0.75 ]]\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'price': [10, 20, 15, 25, 30],\n",
    "    'rating': [4.2, 3.8, 4.5, 3.7, 4.0],\n",
    "    'delivery_time': [20, 30, 25, 40, 35]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d63f04b-c9ad-47c8-bb1c-dcda6f3fd2d5",
   "metadata": {},
   "source": [
    "# ANSWER 6\n",
    "In the context of predicting stock prices, where the dataset contains numerous features related to company financial data and market trends, PCA can be used to reduce the dimensionality of the dataset. This can help in simplifying the model and potentially improve prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95272231-3cad-495a-9ba6-bc3b16ae4596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.03776078e+03 -3.85408150e+01]\n",
      " [-4.67454928e+00  1.94468133e+01]\n",
      " [ 5.04859079e+02  3.90680170e+01]\n",
      " [-5.14217515e+02 -1.66328920e-01]\n",
      " [-1.02372780e+03 -1.98076864e+01]]\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'financial_feature1': [1000, 2000, 1500, 2500, 3000],\n",
    "    'financial_feature2': [500, 800, 700, 900, 1000],\n",
    "    'market_trend1': [10, 15, 12, 20, 18],\n",
    "    'market_trend2': [5, 8, 7, 9, 10]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(df)\n",
    "\n",
    "print(reduced_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b6b4f-b147-490a-8ff1-118d2a58c69d",
   "metadata": {},
   "source": [
    "# ANSWER 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "472aa882-b1ec-406f-9494-b026cb42b86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = [1, 5, 10, 15, 20]\n",
    "\n",
    "# Calculate min and max values\n",
    "min_val = np.min(data)\n",
    "max_val = np.max(data)\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "scaled_data = [(x - min_val) / (max_val - min_val) * 2 - 1 for x in data]\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf866c-b371-4d34-83cc-ba545e7c7727",
   "metadata": {},
   "source": [
    "# ANSWER 8\n",
    "The number of principal components to retain depends on the desired level of dimensionality reduction and the percentage of variance explained by the components. In practice, we aim to retain principal components that collectively explain a significant portion of the data's variance, while still reducing the dimensionality.\n",
    "\n",
    "A common approach is to choose the number of principal components that explain a sufficiently high percentage of the total variance, such as 95% or 99%. To determine the optimal number of components, we can use the cumulative explained variance plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae4f93f7-59b7-4188-a682-af3724b8f926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to retain 95% of variance: 2\n",
      "Number of components to retain 99% of variance: 3\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'height': [170, 165, 180, 175, 160],\n",
    "    'weight': [70, 65, 80, 75, 60],\n",
    "    'age': [30, 25, 35, 28, 32],\n",
    "    'gender': [0, 1, 0, 1, 1],\n",
    "    'blood_pressure': [120, 110, 130, 125, 115]}\n",
    "df = pd.DataFrame(data)\n",
    "pca = PCA()\n",
    "pca.fit(df)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "num_components_95_percent = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "num_components_99_percent = np.argmax(cumulative_variance >= 0.99) + 1\n",
    "\n",
    "print(\"Number of components to retain 95% of variance:\", num_components_95_percent)\n",
    "print(\"Number of components to retain 99% of variance:\", num_components_99_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6134f6-bfe8-4711-a9b7-d65ab754075b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26656d26-8368-4d87-a1b7-d62219202da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
